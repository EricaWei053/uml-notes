\section{Tensor decompositions}
Our goal of tensor decomposition is to take a tensor $T \in (\mathbb{R}^n)^{\otimes d}$ and write it as a sum:
\[T = \sum_{i=1}^r v^{(1)}_i \otimes \dotsm \otimes v^{(d)}_i.\]
In the matrix case, where $d = 2$, this amounts to finding a decomposition of a matrix $M \in \mathbb{R}^{n \times n}$ such that:
\[M = \sum_{i=1}^r u_i v_i^\mathsf{T},\]
where $u_i, v_i \in \mathbb{R}^n$. In the matrix case, there is no unique decomposition of the matrix. Here, $r$ is at most $n$, so we have $2 n^2$ degrees of freedom. Given the matrix $M$, we obtain $n^2$ equations, so in total we have $n^2$ degrees of freedom left. Indeed, notice that given any decomposition, taking any invertible matrix $A \in GL(n;\mathbb{R})$,
\[M = \sum_{i=1}^n u_i v_i^\mathsf{T} = \sum_{i=1}^n (u_i A^\mathsf{T})(v_iA^{-1})^\mathsf{T},\]
where $GL(n;\mathbb{R})$ is an $n^2$-dimensional manifold.

However, for $d \geq 3$ and $r \leq n$, in the tensor case, we have $dn^2$ degrees of freedom but $n^d$ equations, so we have a chance of being able to identify the $v_i^{(j)}$'s from $T$. In fact, we can often use Jennrich's algorithm. 

\subsection{Jennrich's algorithm} Let $T \in (\mathbb{R}^n)^{\otimes 3}$. Suppose that $T$ is rank $r \leq n$, of the form:
\[T = \sum_{i=1}^r u_i \otimes v_i \otimes w_i.\]
Let $U$, $V$, and $W$ be the matrices whose columns are $u_i$, $v_i$, and $w_i$. Assume that $V,W$ are full rank.

Then, draw two vectors $\xi_1, \xi_2 \sim \mathcal{N}(0,\mathrm{Id}_{n\times n})$ from a standard Gaussian on $\mathbb{R}^n$. Compute the matrices $M_1 = T(\xi_1)$ and $M_2 = T(\xi_2)$:
\[M_j = \sum_{i=1}^r \langle \xi_j, u_i\rangle v_i \otimes w_i.\]
In other words, letting $D_j = \mathrm{diag}(\langle \xi_i , u_i\rangle)_{i=1}^r$, we obtain:
\[M_j = VD_jW^\mathsf{T}.\]
We would like to decompose the $M_j$'s, in order to recover $V$ and $W$, allowing us to also solve for $U$. Unlike before, we now have two matrices of the form $VD_1W^\mathsf{T}$ and $VD_2W^\mathsf{T}$. By assumption, $V$ and $W$ are full rank. $D_j$ will essentially always be invertible. And so, we can compute:
\[M_1M_2^+ = V D_1D_2^{-1} V^+.\]
Notice that $v_i$ is an eigenvector to $M_1M_2^+$ with eigenvalue $\langle \xi_1, u_i\rangle / \langle \xi_2, u_i\rangle$. As $M_1 M_2^+$ has exactly $r$ nonzero eigenvalues, their corresponding eigenvectors must be $v_1,\dotsc, v_r$.

\subsection{Orthogonal tensor decomposition}
\begin{definition}
Let $T \in (\mathbb{R}^n)^{\otimes 3}$. An \emph{eigenvector} of $T$ is a unit vector $u \in \mathbb{R}^n$ such that:
\[T(I,u,u) = \lambda u,\]
for some eigenvalue $\lambda \in \mathbb{R}$. 
\end{definition}

Note that eigenvectors for tensors are slightly different from matrix eigenvectors, in that:
\begin{itemize}
    \item if $\lambda_1 = \lambda_2$ for two eigenvectors $v_1,v_2$, their linear combination is not necessarily an eigenvector
    \item on the other hand, $\frac{1}{\lambda_1} v_1 + \frac{1}{\lambda_2} v_2$ is proportional to an eigenvector.
\end{itemize}

\begin{definition}
A \emph{robust eigenvector} of $T$ is $u \in \mathbb{R}^n$ such that there exists an $\epsilon > 0$ such that for all $\theta \in B(u,\epsilon)$, repeated iteration of the map
\[\bar{\theta} \mapsto \frac{T(I,\bar{\theta},\bar{\theta})}{\|T(I,\bar{\theta},\bar{\theta})\|}\]
starting from $\theta$ converges to $u$.
\end{definition}

\begin{theorem}
Let $T$ be odeco.
\begin{itemize}
    \item the set of $\theta \in \mathbb{R}^n$ which do not converge to some $v_i$ under repeated iterations of the above map has measure zero.
    \item the set of robust eigenvectors of $T$ is equal to $\{v_1,\dotsc, v_n\}$.
\end{itemize}
\end{theorem}

\begin{lemma}
Let $T \in \bigotimes^3 \mathbb{R}^n$ have an orthogonal decomposition. For a vector $\theta_0 \in \mathbb{R}^n$, suppose that the set of numbers $|\lambda_1 v_1^\mathsf{T}\theta_0|,\dotsc, |\lambda_k v_k ^\mathsf{T} \theta_0|$ has a unique largest element. WLOG, let $|\lambda_1 v_1^\mathsf{T}\theta_0|$ be the largest and the corresponding term for be the second largest. Then:
\[\|v_1 - \theta_1 \|^2 \leq \left(2\lambda_1^{2} \sum_{i=2}^k \lambda_i^{-2}\right)\abs{\frac{\lambda_2v_2^\mathsf{T} \theta_0}{\lambda_1v_1^\mathsf{T}\theta_0}}^{2^{t+1}}\]
\end{lemma}

\section{Parameter estimation}

\begin{itemize}
    \item Describe general algorithm for latent variable models through method of moments
    \item Give specific moments for LSI, LDA, GMMs
    \item Prove bounds (contrast with EM, with not guarantees)
\end{itemize}

